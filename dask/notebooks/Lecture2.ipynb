{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f76699",
   "metadata": {},
   "source": [
    "# Dask Data Structures\n",
    "\n",
    "Dask provides several Pythonic data structures designed to handle and manipulate data that exceeds our local memory capacity:\n",
    "\n",
    "- `dask.bag`: A distributed generic Python list of objects, analogous to a PySpark RDD.\n",
    "- `dask.array`: Distributed NumPy arrays.\n",
    "- `dask.dataframe`: Distributed pandas dataframes, offering functionality similar to pandas but capable of handling larger-than-memory datasets.\n",
    "\n",
    "All these high-level data structure APIs are optimized to leverage the Directed Acyclic Graph (DAG) optimization features of the Dask scheduler. Consequently, they rely on lazy computation, allowing for efficient execution of operations on large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21d093",
   "metadata": {},
   "source": [
    "## Start the Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf23cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    " \n",
    "# use the provided master\n",
    "client = Client('dask-scheduler:8786')\n",
    "    \n",
    "# print the status of the client    \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08409bcd",
   "metadata": {},
   "source": [
    "## Dask Bag\n",
    "\n",
    "Bags are highly versatile and flexible data structures in Dask.\n",
    "\n",
    "Dask Bag offers a level of flexibility similar to PySpark's RDDs. They serve as parallelized collections of objects, similar to Python's built-in `list`, capable of holding any Python objects, whether they're custom classes or built-in types. \n",
    "\n",
    "This flexibility allows for the storage of complex data structures like raw text or nested JSON data, which can be easily navigated.\n",
    "\n",
    "Due to this versatility, Dask bags are commonly employed to parallelize simple computations on unstructured or semi-structured data, such as text data, log files, JSON records, or user-defined Python objects. They facilitate MapReduce-like approaches for loading, inspecting, filtering, and processing arbitrary datasets, whether structured or unstructured.\n",
    "\n",
    "Dask Bag provides operations such as `map`, `filter`, `groupby`, and aggregations on collections of Python objects. It accomplishes these tasks in parallel using Python iterators, resembling a parallel version of `itertools`.\n",
    "\n",
    "After completing an initial stage of data preparation with Dask Bag, it's often customary to reduce and convert the data into more suitable data structures, such as Dask Arrays or Dask DataFrames, which will be covered in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd63846",
   "metadata": {},
   "source": [
    "### Create and Take from a Bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced8b7d",
   "metadata": {},
   "source": [
    "We can create a `Bag` from various data sources, including Python sequences, files, and cloud storage services like Amazon AWS S3, among others. \n",
    "For a comprehensive overview on accessing remote data from Distributed File Systems, S3, and other sources, please refer to the official documentation [here](https://docs.dask.org/en/stable/how-to/connect-to-remote-data.html).\n",
    "\n",
    "Additionally, we can create a Bag from a function declared as `delayed`. \n",
    "This approach allows us to generate data within a distributed application and subsequently access it using the Bag API before computing a result.\n",
    "\n",
    "The data within the Bag is partitioned into blocks, typically with multiple items per block. The number of partitions (`npartitions`) depends on factors such as the dataset size, cluster resources, and our specified partitioning strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba350e",
   "metadata": {},
   "source": [
    "Let's start by creating a simple data `bag` from a Python list.\n",
    "\n",
    "In Python, we can easily create data from a list. \n",
    "Since Python is a dynamically typed language, the list can contain a variety of data types such as integers, strings, or even objects.\n",
    "\n",
    "For example, we can create a simple array of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "# create a Dask Bag from a Python sequence\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# create a Dask Bag from the sequence with 4 partitions\n",
    "b = db.from_sequence(data, npartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905079b",
   "metadata": {},
   "source": [
    "As previously mentioned, Dask data structures embody the lazy programming paradigm.\n",
    "The data is thus not yet stored on the cluster, as we have not performed an operation such as `compute`.\n",
    "\n",
    "In general, we don't want to retrieve the entire data stored on the cluster, but we might want to inspect a few elements.\n",
    "We can do that with the `take(n_elements)` method.\n",
    "The returned data will be a tuple containing the first `n_elements` of the Bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49904962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the first 3 elements of the Bag\n",
    "b.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266bd9c",
   "metadata": {},
   "source": [
    "Data from text files can be extracted in Dask by providing either a list of all files or using the `*` wildcard.\n",
    "\n",
    "By default, the resulting Dask Bag will have one item per line and one file per partition. \n",
    "It's important to be mindful of partitioning when working with large datasets to avoid inefficient processing.\n",
    "\n",
    "Dask provides automatic handling of standard compression libraries (e.g., gzip, bz2, xz) when reading text files. The compression type can be inferred from the file name extension or explicitly specified using the `compression='gzip'` keyword argument.\n",
    "\n",
    "For example, to load a number of compressed files from a local folder into a Dask Bag:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d025db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files provided\n",
    "! ls datasets/accounts_json/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3f608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# read text files from the specified directory and use gzip compression\n",
    "b = db.read_text(os.path.join('datasets',\n",
    "                              'accounts_json',\n",
    "                              'accounts.*.json.gz'),\n",
    "                 files_per_partition=4)\n",
    "\n",
    "# take the first element from the bag\n",
    "example = b.take(1)\n",
    "\n",
    "# print the type of the example variable\n",
    "print(type(example))\n",
    "\n",
    "# print the example variable\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9554e",
   "metadata": {},
   "source": [
    "`Bag` objects in Dask support standard functional APIs, such as `map`, `filter`, `groupby`, and more. These operations create new bags, allowing for chaining multiple operations together to manipulate the data until the desired result is obtained.\n",
    "\n",
    "To execute the computations on a `Bag` object and obtain the final result, we can use the `.compute()` method, similar to how we handle `delayed` objects.\n",
    "\n",
    "Since a `Bag` is inherently a delayed object, there's no need to explicitly specify that the functions applied to the dataset are further delayed. Dask handles the delayed execution transparently, allowing for efficient processing of large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a silly function to check if a number is even\n",
    "def is_even(n):\n",
    "    return n % 2 == 0\n",
    "\n",
    "# create a bag from a sequence of numbers\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# 1. filter the bag elements to retain only the even ones\n",
    "# 2. then, map a lambda function to square each even number\n",
    "c = b.filter(is_even).map(lambda x: x ** 2)\n",
    "\n",
    "# print the resulting bag\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the computational graph\n",
    "c.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbc743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the results\n",
    "c.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9eb74f",
   "metadata": {},
   "source": [
    "One of the crucial parameters when optimizing a computing task on a cluster is ensuring the effective utilization of available computing resources. This involves rewriting the computational task to be distributed efficiently and optimizing the number of partitions used to store data and perform map-like data transformations.\n",
    "\n",
    "### Considerations for Partitioning\n",
    "\n",
    "Consider the following (\"extreme\") scenarios:\n",
    "- Having only one partition with 100 available CPUs\n",
    "- Having 10,000 tiny partitions with 3 available CPUs\n",
    "\n",
    "#### Inefficient Single Partition\n",
    "Having only one partition with a large number of available CPUs would be inefficient as there would be no parallelization at all.\n",
    "\n",
    "#### Excessive Tiny Partitions\n",
    "Conversely, having an excessive number of tiny partitions with few available CPUs each would likely be inefficient due to the overhead of starting and stopping computation on each individual tiny partition.\n",
    "\n",
    "#### Optimal Partitioning Strategy\n",
    "The optimal number of partitions depends on factors such as the amount of shuffling required, especially for operations like `groupby`. For instance, having a moderate number of partitions with a balanced distribution of available CPUs might be more efficient than having a smaller or larger number of partitions. However, the optimal choice will ultimately depend on your specific workload and cluster characteristics.\n",
    "\n",
    "#### Experimentation and Fine-Tuning\n",
    "Determining the optimal partitioning strategy is a task that requires experimentation and educated guesses based on the available processing units in your cluster. There is no one-size-fits-all answer to this question. You'll need to experiment and fine-tune your partitioning strategy based on your specific workload, computational requirements, and cluster characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new bag with the same data as in the previous example\n",
    "# but a different number of partitions\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "                     npartitions=3)\n",
    "\n",
    "# 1. filter the bag elements to retain only the even ones\n",
    "# 2. then, map a lambda function to square each even number\n",
    "c = b.filter(is_even).map(lambda x: x ** 2)\n",
    "\n",
    "# visualize the computation graph\n",
    "c.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the results\n",
    "c.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c66468",
   "metadata": {},
   "source": [
    "## Exercise 1 - Open and Preprocess JSON Data\n",
    "\n",
    "We'll start with a dummy dataset of gzipped JSON data located in your data directory. This dataset simulates data that you might collect from a document store database (e.g., MongoDB) or by scraping a website using a dedicated API.\n",
    "\n",
    "Each line of each document is a JSON-encoded dictionary with the following keys:\n",
    "\n",
    "- `id`: Unique identifier of the customer.\n",
    "- `name`: Name of the customer.\n",
    "- `transactions`: A list of key-value pairs in the form of `transaction-id` and `amount` pairs, representing each transaction made by the customer in that file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07021218",
   "metadata": {},
   "source": [
    "1. **Create a Bag reading out the dataset from the text files**\n",
    "2. **Map the `json.loads` function on each message to convert the records in the form of Python dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create a Dask Bag from the files\n",
    "\n",
    "# load up the files from dask directly in a bag\n",
    "\n",
    "# take the first 3 elements from the Bag\n",
    "\n",
    "# print the examples\n",
    "\n",
    "# print the type of examples and the type of its first element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 2. read the data from the JSON format\n",
    "\n",
    "# map each line to a JSON object using json.loads\n",
    "\n",
    "# take the first 3 elements from the Bag\n",
    "\n",
    "# pretty-print the examples\n",
    "\n",
    "# print the type of examples and the type of its first element\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9df035",
   "metadata": {},
   "source": [
    "Once the JSON data is mapped into the appropriate Python objects (dictionaries, lists, etc.) within a Dask Bag, we can perform specific operations by creating small Python functions to run on our data.\n",
    "\n",
    "The most fundamental operations we can perform on a Dask Bag include:\n",
    "\n",
    "- `map`: Apply a function to each element.\n",
    "- `filter`: Retain only the elements that satisfy a given function.\n",
    "- `pluck`: Select a specific nested field, such as `element[field]` from a Python dictionary.\n",
    "- `flatten`: Unfold the dictionary into a list-like object.\n",
    "\n",
    "These operations serve as building blocks for manipulating and transforming the data within a Dask Bag, providing flexibility in data processing pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fc7fe",
   "metadata": {},
   "source": [
    "#### 1. Compute the average number of transactions for each entry of a user named \"Alice\"\n",
    "\n",
    "To achieve this task using Dask Bag operations, we'll follow these steps:\n",
    "\n",
    "1. Filter the dataset to retain only entries for users named \"Alice\".\n",
    "2. Map each entry to extract the number of transactions.\n",
    "3. Compute the average number of transactions for each entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f076cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND count the total number of transactions for each entry in the dataset \n",
    "\n",
    "# function reformatting each record with the new information\n",
    "def count_transactions(d):\n",
    "    return \n",
    "\n",
    "db_js.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND count the total number of transactions (as 'count') for each entry in the dataset \n",
    "# AND return only the 'count' values\n",
    "db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f06cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND count the total number of transactions (as 'count') for each entry in the dataset \n",
    "# AND return only the 'count' values\n",
    "# AND compute the average of the counts\n",
    "db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc7b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph of the tasks composing the job\n",
    "db_js."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc0880",
   "metadata": {},
   "source": [
    "#### 2. Compute the average amount of transactions users named \"Alice\"\n",
    "\n",
    "To compute the average amount for all transactions made by users named \"Alice\" using Dask Bag operations, we'll follow these steps:\n",
    "\n",
    "1. Filter and pluck the dataset to retain only entries for users named \"Alice\".\n",
    "2. Flatten the list of transaction amounts.\n",
    "3. Compute the average of all transaction amounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8bc3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "db_js.filter(lambda record: record['name'] == 'Alice')\\\n",
    "     .pluck('transactions')\\\n",
    "     .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333441ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND flatten and pluck to return only the \"amount\" in a bag\n",
    "db_js.filter(lambda record: record['name'] == 'Alice')\\\n",
    "     .pluck('transactions')\\\n",
    "     .flatten()\\\n",
    "     .pluck('amount')\\\n",
    "     .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d440031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the relevant transactions\n",
    "# AND flatten and pluck to return only the \"amount\" in a bag\n",
    "# AND compute the average of all transactions amounts\n",
    "db_js.filter(lambda record: record['name'] == 'Alice')\\\n",
    "     .pluck('transactions')\\\n",
    "     .flatten()\\\n",
    "     .pluck('amount')\\\n",
    "     .mean()\\\n",
    "     .compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph of the tasks composing the job\n",
    "db_js.filter(lambda record: record['name'] == 'Alice')\\\n",
    "     .pluck('transactions')\\\n",
    "     .flatten()\\\n",
    "     .pluck('amount')\\\n",
    "     .mean()\\\n",
    "     .visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb1002",
   "metadata": {},
   "source": [
    "### Additional operations on Dask Bags: Groupby and Foldby\n",
    "\n",
    "Additional standard operations on Dask Bags can be performed using the `groupby` and `foldby` methods, together with data aggregation functions.\n",
    "\n",
    "#### Groupby\n",
    "The `groupby` method shuffles the data so that all items with the same key are grouped together in the same key-value pair. This operation is useful for tasks that involve grouping data by a specific key.\n",
    "\n",
    "#### Foldby\n",
    "The `foldby` method walks through the data and accumulates a result per key. It combines the functionality of `groupby` and a reduction operation, making it suitable for efficient parallel split-apply-combine tasks. While more complex to use, `foldby` significantly reduces computational time compared to `groupby`, especially for tasks involving heavy data shuffling.\n",
    "\n",
    "#### Considerations\n",
    "It's important to note that operations involving heavy data shuffling, such as `groupby`, can be computationally expensive as they require moving data across workers. In such cases, using the `foldby` method provides a more efficient alternative.\n",
    "\n",
    "Consider using the `foldby` method whenever possible to optimize performance and minimize data shuffling overhead, especially in large-scale distributed computing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461dc7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_data = ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith', 'Frank']\n",
    "\n",
    "# create a bag from the list of names\n",
    "b = db.from_sequence(names_data)\n",
    "\n",
    "# group names by length\n",
    "res = b.groupby(len) \n",
    "\n",
    "# visualize this \"simple\" graph before computing the results\n",
    "res.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f646a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the results\n",
    "res.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a261a",
   "metadata": {},
   "source": [
    "When the result of a `groupby` operation in Dask is a tuple and we need to apply functions to the elements of these tuples, we can use the `starmap` function.\n",
    "\n",
    "The `starmap` function in Dask allows us to apply a function using argument tuples, similar to what the standard `itertools.starmap` does in Python.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple bag from a list of integers\n",
    "b = db.from_sequence(list(range(10)))\n",
    "\n",
    "# group numbers into even/odd groups\n",
    "b.groupby(lambda x: x % 2).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the max value for all elements in each group\n",
    "b.groupby(lambda x: x % 2)\\\n",
    " .starmap(lambda k, v: (k, max(v)))\\\n",
    " .compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the sum of the elements in each group\n",
    "b.groupby(lambda x: x % 2)\\\n",
    " .starmap(lambda k, v: (k, sum(v)))\\\n",
    " .compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph of this latest \"extremely simple\" computation\n",
    "res = b.groupby(lambda x: x % 2)\\\n",
    "       .starmap(lambda k, v: (k, sum(v)))\n",
    "\n",
    "res.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9a221",
   "metadata": {},
   "source": [
    "### Understanding `foldby` in Dask\n",
    "\n",
    "`foldby` in Dask can initially seem peculiar, but it shares similarities with functions in other libraries:\n",
    "\n",
    "- [`toolz.reduceby`](http://toolz.readthedocs.io/en/latest/streaming-analytics.html#streaming-split-apply-combine)\n",
    "- [`pyspark.RDD.combineByKey`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.combineByKey.html)\n",
    "\n",
    "When using `foldby`, you need to provide:\n",
    "\n",
    "1. A **key function to group elements** (similar to `groupby`).\n",
    "2. Either:\n",
    "    - A **binary operator** (function that takes 2 elements and returns 1 of the same type) that performs **reduction within each group**.\n",
    "    - Or a **combine binary operator** that can **combine the results of two `reduce` calls on different parts of your dataset**.\n",
    "\n",
    "\n",
    "In Dask, a `foldby` call like this:\n",
    "\n",
    "```python\n",
    "dask_bag.foldby(key, binop, init)\n",
    "```\n",
    "is equivalent to a combination of two operations: `groupby` and `reduce`:\n",
    "\n",
    "```python\n",
    "def reduction(group):\n",
    "    return reduce(binop, group, init)\n",
    "\n",
    "dask_bag.groupby(key).map(lambda (k, v): (k, reduction(v)))\n",
    "```\n",
    "\n",
    "The reduction operation must be associative and is executed in parallel within each partition of the dataset. The intermediate results are then combined using the `combine` binary operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117495d",
   "metadata": {},
   "source": [
    "Let's re-write the equivalent group-by + starmap operation with a foldby call\n",
    "\n",
    "```python\n",
    "b.groupby(lambda x: x % 2)\\\n",
    " .starmap(lambda k, v: (k, sum(v)))\\\n",
    " .compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple bag from a list of integers\n",
    "b = db.from_sequence(list(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eaf754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby even/odd numbers with a foldby and find the total sum per group\n",
    "#\n",
    "#   write down a **binary filter function** to select only even or odd numbers\n",
    "#   write down a **reduce-like operation** to sum all elements\n",
    "is_even = lambda x: x % 2 == 0 \n",
    "add     = lambda x, y: x + y\n",
    "b.foldby(key=is_even, \n",
    "         binop=add, \n",
    "         initial=0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the graph and compare it \n",
    "# with the groupby implementation\n",
    "# \n",
    "# the `split_every` option instructs foldby to group \n",
    "# partitions into groups of this size while performing the reduction.\n",
    "# (`split_every` defaults to 8)\n",
    "b.foldby(is_even, \n",
    "         add, \n",
    "         initial=0, \n",
    "         split_every=8).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5dc49",
   "metadata": {},
   "source": [
    "## Exercise 2 - Account Data\n",
    "\n",
    "In this exercise, we'll work with account data to get the total number of users with the same name from the dataset. We'll compare the computational time of achieving this task using both the `groupby` and `foldby` functions in Dask.\n",
    "\n",
    "- Take a moment to look at the `foldby` API documentation [here](https://docs.dask.org/en/latest/generated/dask.bag.Bag.foldby.html#dask.bag.Bag.foldby).\n",
    "\n",
    "1. **Using Groupby Function:**\n",
    "   - Utilize the `groupby` function to group users by their name and count the number of users with the same name.\n",
    "   - Measure the computational time required for this operation.\n",
    "\n",
    "2. **Using Foldby Function:**\n",
    "   - Explore the `foldby` function in the Dask Bag API to achieve the same task.\n",
    "   - Measure the computational time required for this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b803d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# groupby on the 'name' key \n",
    "# count the number of items in each group\n",
    "result_groupby = db_js."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9b4bcd",
   "metadata": {},
   "source": [
    "Let's inspect what `groupby` is doing in Dask..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(result_groupby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbafd2",
   "metadata": {},
   "source": [
    "What about `foldby`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# foldby on 'name' key \n",
    "# increment by one each time we see an element (binop function)\n",
    "# use a final combination function with add (combine function)\n",
    "result_foldby = db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef768fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(result_foldby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd7bee",
   "metadata": {},
   "source": [
    "## Exercise 3 - Computing Total Transfers Amount per Name\n",
    "\n",
    "In this exercise, we'll work with account data to compute the total transfers amount per each name using a foldby operation in Dask.\n",
    "\n",
    "1. **Create a Function for Summing Amounts:**\n",
    "   - Create a function that takes an input dictionary containing the name and transactions, and produces the sum of the amounts.\n",
    "   - For example:\n",
    "     ```python\n",
    "     {'name': 'Alice', 'transactions': [{'amount': 1, 'id': 123}, {'amount': 2, 'id': 456}]}\n",
    "     ```\n",
    "     The sum of the amounts for this input would be 3.\n",
    "\n",
    "2. **Modify Binary Operator for Foldby:**\n",
    "   - Modify the binary operator of the `foldby` examples to accumulate the sum of the transferred amounts instead of counting the number of entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ab46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the sum_transactions values together\n",
    "def add_values(tot, _):\n",
    "    return \n",
    "\n",
    "# compute the sum of transaction amounts per name\n",
    "def sum_amount(d):\n",
    "    \n",
    "    return \n",
    "\n",
    "# apply sum_amount function to each item in db_js\n",
    "# perform foldby operation on 'name' key to accumulate the sum of transactions for each name\n",
    "result_foldby = db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef61681",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_foldby.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_foldby = result_foldby.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19245641",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(result_foldby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a302c",
   "metadata": {},
   "source": [
    "## From Bag to pre-processed output datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c10ab",
   "metadata": {},
   "source": [
    "Dask Bags are often used as an entry point to ingest, decode, and preprocess data before further processing or analysis. Once the data has been processed and transformed, it's desirable to convert the Dask Bag into a structured data format, such as a Dask DataFrame, for easier manipulation and analysis.\n",
    "\n",
    "Dask provides several methods to convert Bags into output data objects, including:\n",
    "\n",
    "- `to_textfiles`: Writes the Bag data to multiple text files.\n",
    "- `to_avro`: Writes the Bag data to Avro files.\n",
    "- `to_delayed`: Converts the Bag into a list of delayed objects.\n",
    "\n",
    "By far, the most widely used approach in data preprocessing using Dask Bags follows the Extract-Transform-Load (ETL) process:\n",
    "\n",
    "1. **Extract**: Raw data is extracted from the original input source.\n",
    "2. **Transform**: The extracted data is transformed by applying functions to filter, reduce, or create features from the original (often messy) dataset.\n",
    "3. **Load**: The transformed and cleansed dataset is loaded into a database or further data processing pipeline based on structured data.\n",
    "\n",
    "\n",
    "To convert a Dask Bag into a Dask DataFrame, the dataset needs to be flattened and normalized before invoking the `to_dataframe` function on the Bag. This flattening and normalization process ensures that the data is in a structured format that can be easily loaded into a DataFrame.\n",
    "\n",
    "This conversion process is similar to converting from RDD to a Spark DataFrame and is a common operation in data processing workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b9cdd",
   "metadata": {},
   "source": [
    "## Flattening Deeply Nested Account Data\n",
    "\n",
    "As a purely illustrative example, our account data is deeply nested and not suitable for being transformed into a table-like DataFrame structure. \n",
    "\n",
    "We may want to retain only the first transaction per customer or retain other features such as the max-amount transfer or an aggregated quantity per user.\n",
    "\n",
    "To achieve this, we can flatten the dataset by mapping a dedicated function. This function will extract the desired features from the nested structure and create a flattened representation suitable for further processing or conversion into a Dask DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4964a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print one element of the json bag\n",
    "pprint( db_js.take(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to flatten the nested structure of the record\n",
    "# and extract specific fields\n",
    "def dummy_flatten(record):\n",
    "    return {\n",
    "        'id': record['id'],\n",
    "        'name': record['name'],\n",
    "        'first_transaction_id': record['transactions'][0]['transaction-id'],\n",
    "        'first_transaction_amount': record['transactions'][0]['amount']\n",
    "    }\n",
    "\n",
    "# apply the dummy_flatten function to each record in db_js\n",
    "# and take the first element from the resulting Bag\n",
    "pprint(db_js.map(dummy_flatten).take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cccd9c-39c2-4166-b941-965ace8889d9",
   "metadata": {},
   "source": [
    "To create a Dask distributed DataFrame, we can use the `to_dataframe` method on the flattened Dask Bag.\n",
    "\n",
    "The Dask DataFrame is still a Dask distributed object, i.e.:\n",
    "\n",
    "- **Partitioned Across Workers**: Like other Dask data structures, Dask DataFrames are partitioned across the workers in the cluster. This allows for parallel processing of data.\n",
    "- **Lazy Operations**: Operations performed on Dask DataFrames are lazy, meaning that they are not executed immediately. Instead, a task graph is created, and computations are deferred until the result is explicitly needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Dask DataFrame from the flattened Bag\n",
    "dd = db_js.map(dummy_flatten).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb792f7a-bd24-4576-9f33-48db9bd7272e",
   "metadata": {},
   "source": [
    "Unlike a Pandas DataFrame, printing a Dask DataFrame won't show its records but rather its structure and the number of partitions it has been divided into.\n",
    "\n",
    "- **Structure Display**: When you print a Dask DataFrame, you'll see its structure, including column names, data types, and the number of partitions.\n",
    "- **Absence of Records**: The actual data records are not shown in their entirety. Displaying all records would be equivalent to collecting all data in the scheduler and passing it to the client, which is similar to the `collect` operation. This could lead to performance issues with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70535f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the DataFrame\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0ca15-ab83-459e-b0c2-60fb054fd467",
   "metadata": {},
   "source": [
    "In Dask DataFrames, issuing the `head` or `tail` methods triggers computation and retrieves results from the cluster. \n",
    "\n",
    "This behavior is different from the printing of the DataFrame structure, which only displays metadata without fetching the actual data.\n",
    "\n",
    "- **Computation Trigger**: When you call the `head` or `tail` methods on a Dask DataFrame, Dask will compute a small portion of the DataFrame to display. This computation is performed on the cluster, and the results are then returned to the client for display.\n",
    "- **Performance Consideration**: It's important to consider the performance implications of calling `head` or `tail`, especially for large datasets. Triggering computation for a large portion of the DataFrame can lead to significant overhead, as it involves transferring data between workers and the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb83c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the first 10 elements of the dask dataframe\n",
    "dd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac68dc",
   "metadata": {},
   "source": [
    "## Stop client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045177d",
   "metadata": {},
   "source": [
    "Finally, use `docker compose down` to stop and clear all running containers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
